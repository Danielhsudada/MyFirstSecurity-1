# 教科書

```
Python大數據特訓班：資料自動化收集、整理、分析、儲存與應用實戰(附近300分鐘影音教學/範例程式)
作者： 文淵閣工作室   出版社：碁峰  出版日期：2018/07/11
https://www.books.com.tw/products/0010792124
```
```
Python：網路爬蟲與資料分析入門實戰
作者： 林俊瑋, 林修博    出版社：博碩   出版日期：2018/10/04
https://www.books.com.tw/products/0010800867

https://github.com/jwlin/py-scraping-analysis-book
https://github.com/jwlin/web-crawler-tutorial
```

```
https://realpython.com/tutorials/web-scraping/
```
# Python and security (web testing)
```
Learning Python Web Penetration Testing
Christian Martorella June 2018
https://www.packtpub.com/networking-and-servers/learning-python-web-penetration-testing
```

```
Python For Offensive PenTest
Hussam Khrais April 2018
https://www.packtpub.com/networking-and-servers/python-offensive-pentest
```
```
Python Penetration Testing Essentials - Second Edition
Mohit May 2018
https://www.packtpub.com/networking-and-servers/python-penetration-testing-essentials-second-edition
```
```
https://github.com/rivallu/python

```

# 使用requests模組

Python3
```
import requests
url = 'http://www.google.com'
html = requests.get(url)
html.encoding="utf-8"
if html.status_code == requests.codes.ok:
    print(html.text)
```
Python2
```
import requests
payload= {'url':'http://www.edge-security.com'}
r=requests.get('http://httpbin.org/redirect-to',params=payload)
print "Status code:"
print "\t *" + str(r.status_code)
```

### 使用GET 請求::查詢參數
```
import requests
payload = {'key1': 'value1', 'key2': 'value2'}

html = requests.get("http://httpbin.org/get", params=payload)
print(html.url) # http://httpbin.org/get?key1=value1&key2=value2
```
```
#!/usr/bin/env
import requests
r =  requests.post('http://httpbin.org/post',data={'name':'packt'})
print r.url
print 'Status code:' + '\t[-]' + str(r.status_code) + '\n'
print 'Server headers'
print '****************************************'
for x in r.headers:
    print '\t' + x + ' : ' + r.headers[x]
print '****************************************\n'
print r.text
```
### 使用 POST請求::查詢參數
```
import requests
payload = {'key1': 'value1', 'key2': 'value2'}
html = requests.post("http://httpbin.org/post", data=payload)
print(html.text)
```

# 使用BeautifulSoup模組解析HTML
```
import requests
from bs4 import BeautifulSoup
url = 'http://www.e-happy.com.tw'
html = requests.get(url)
sp = BeautifulSoup(html.text, 'html.parser')
print(sp.title)
```

```
html = """
<html><head><title>網頁標題</title></head>
<p class="header"><h2>文件標題</h2></p>
<div class="content">
    <div class="item1">
        <a href="http://example.com/one" class="red" id="link1">First</a>
        <a href="http://example.com/two" class="red" id="link2">Second</a>
    </div>
    <a href="http://example.com/three" class="blue" id="link3">
        <img src="http://example.com/three.jpg">Third
    </a>
</div>
"""

from bs4 import BeautifulSoup
sp = BeautifulSoup(html,'html.parser') 

print(sp.title) # <title>網頁標題</title>

print(sp.find('h2')) # <h2>文件標題</h2>

print(sp.find_all('a')) 
print(sp.find_all("a", {"class":"red"}))

data1=sp.find("a", {"href":"http://example.com/one"})
print(data1.text) # First

data2 = sp.select("#link1") 
print(data2[0].text) # First
print(data2[0].get("href")) # http://example.com/one
print(data2[0]["href"])     # http://example.com/one

print(sp.find_all(['title','h2'])) # [<title>網頁標題</title>, <h2>文件標題</h2>]

print(sp.select('div img')[0]['src']) # http://example.com/three.jpg
```
```
import requests
from bs4 import BeautifulSoup
payload = {
   'from': 'https://www.ptt.cc/bbs/Gossiping/index.html',
	'yes': 'yes'
}
headers = {
	'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.87 Safari/537.36'
}
rs = requests.Session()
rs.post('https://www.ptt.cc/ask/over18', data=payload, headers=headers)
res = rs.get('https://www.ptt.cc/bbs/Gossiping/index.html', headers=headers)

soup = BeautifulSoup(res.text, 'html.parser')
items = soup.select('.r-ent')
for item in items:
    print(item.select('.date')[0].text, item.select('.author')[0].text, item.select('.title')[0].text)
```

# Python：網路爬蟲與資料分析入門實戰

待分析的網站:http://blog.castman.net/py-scraping-analysis-book/ch1/connect.html

Ch1:p10
```
import requests
from bs4 import BeautifulSoup

resp = requests.get('https://jwlin.github.io/py-scraping-analysis-book/ch1/connect.html')
soup = BeautifulSoup(resp.text, 'html.parser')
print(soup.find('h1').text)
```
Ch1:p30===錯誤處理
```
import requests
from bs4 import BeautifulSoup


def main():
    url1 = 'http://jwlin.github.io/py-scraping-analysis-book/ch1/connect.html'
    bad_url = 'http://non-existed.domain/connect.html'
    text1 = get_tag_text(url1, 'h1')
    print(text1)
    text2 = get_tag_text(url1, 'h2')
    print(text2)
    text3 = get_tag_text(bad_url, 'h1')
    print(text3)


def get_tag_text(url, tag):
    try:
        resp = requests.get(url)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, 'html.parser')
            return soup.find(tag).text
    except Exception as e:
        print('Exception: %s' %(e))
    return None


if __name__ == '__main__':
    main()
```
### Ch2:Beautiful Soup 講解與網頁解構
```
2-1 不要重複造輪子：寫爬蟲之前
2-2 Beautiful Soup 重要功能 (find(), find_all(), .text, .stripped_strings)
2-3 網頁結構巡覽（parent, children, siblings）
2-4 正規表示式 (Regular Expression)
```

### ch2-2: BeautifulSoup的重要函式:find/find_all

待分析的網站:http://jwlin.github.io/py-scraping-analysis-book/ch2/blog/blog.html
```
import requests
from bs4 import BeautifulSoup

def main():
    resp = requests.get('http://jwlin.github.io/py-scraping-analysis-book/ch2/blog/blog.html')
    soup = BeautifulSoup(resp.text, 'html.parser')

    # 取得第一篇 blog (h4)
    print(soup.find('h4'))
    print(soup.h4)  # 與上一行相等

    # 取得第一篇 blog 主標題
    print(soup.h4.a.text)

    # 取得所有 blog 主標題, 使用 tag
    main_titles = soup.find_all('h4')
    for title in main_titles:
        print(title.a.text)

    # 取得所有 blog 主標題, 使用 class
    # 以下寫法皆相同:
    # soup.find_all('h4', 'card-title')
    # soup.find_all('h4', {'class': 'card-title'})
    # soup.find_all('h4', class_='card-title')
    main_titles = soup.find_all('h4', 'card-title')
    for title in main_titles:
        print(title.a.text)

    # 使用 key=value 取得元件
    print(soup.find(id='mac-p'))

    # 當 key 含特殊字元時, 使用 dict 取得元件
    # print(soup.find(data-foo='mac-foo'))  # 會導致 SyntaxError
    print(soup.find('', {'data-foo': 'mac-foo'}))

    # 取得各篇 blog 的所有文字
    divs = soup.find_all('div', 'content')
    for div in divs:
        # 方法一, 使用 text (會包含許多換行符號)
        #print(div.text)
        # 方法二, 使用 tag 定位
        #print(div.h6.text.strip(), div.h4.a.text.strip(), div.p.text.strip())
        # 方法三, 使用 .stripped_strings
        print([s for s in div.stripped_strings])


if __name__ == '__main__':
    main()
```

### 2-3 網頁結構巡覽（parent, children, siblings）
```
待分析的網站:http://jwlin.github.io/py-scraping-analysis-book/ch2/table/table.html

要完成的任務:
[1]計算課程平均價格
[2]取得每一列所有欄位資訊: 
```
```
import requests
from bs4 import BeautifulSoup


def main():
    resp = requests.get('http://jwlin.github.io/py-scraping-analysis-book/ch2/table/table.html')
    soup = BeautifulSoup(resp.text, 'html.parser')

    # 計算課程均價
    # 取得所有課程價錢: 方法一, 使用 index
    prices = []
    rows = soup.find('table', 'table').tbody.find_all('tr')
    for row in rows:
        price = row.find_all('td')[2].text
        prices.append(int(price))
    print(sum(prices)/len(prices))

    # 取得所有課程價錢: 方法二, <a> 的 parent (<td>) 的 previous_sibling
    prices = []
    links = soup.find_all('a')
    for link in links:
        price = link.parent.previous_sibling.text
        prices.append(int(price))
    print(sum(prices) / len(prices))

    # 取得每一列所有欄位資訊: find_all('td') or row.children
    rows = soup.find('table', 'table').tbody.find_all('tr')
    for row in rows:
        all_tds = row.find_all('td')  # 方法一: find_all('td')
        # all_tds = [td for td in row.children]  # 方法二: 找出 row (tr) 所有的直接 (下一層) children
        # 以下執行時會報錯, 因為最後一列的 <a> 沒有 'href' 屬性
        # print(all_tds[0].text, all_tds[1].text, all_tds[2].text, all_tds[3].a['href'], all_tds[3].a.img['src'])
        # 取得 href 屬性前先檢查其是否存在
        if 'href' in all_tds[3].a.attrs:
            href = all_tds[3].a['href']
        else:
            href = None
        print(all_tds[0].text, all_tds[1].text, all_tds[2].text, href, all_tds[3].a.img['src'])

    # 取得每一列所有欄位文字資訊: stripped_strings
    rows = soup.find('table', 'table').tbody.find_all('tr')
    for row in rows:
        print([s for s in row.stripped_strings])


if __name__ == '__main__':
    main()
```
### 2-4 正規表示式 (Regular Expression)
```
import requests
import re
from bs4 import BeautifulSoup


def main():
    resp = requests.get('http://jwlin.github.io/py-scraping-analysis-book/ch2/blog/blog.html')
    soup = BeautifulSoup(resp.text, 'html.parser')

    # 找出所有 'h' 開頭的標題文字
    titles = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
    for title in titles:
        print(title.text.strip())

    # 利用 regex 找出所有 'h' 開頭的標題文字
    for title in soup.find_all(re.compile('h[1-6]')):
        print(title.text.strip())

    # 找出所有 .png 結尾的圖片
    imgs = soup.find_all('img')
    for img in imgs:
        if 'src' in img.attrs:
            if img['src'].endswith('.png'):
                print(img['src'])

    # 利用 regex 找出所有 .png 結尾的圖片
    for img in soup.find_all('img', {'src': re.compile('\.png$')}):
        print(img['src'])

    # 找出所有 .png 結尾且含 'beginner' 的圖片
    imgs = soup.find_all('img')
    for img in imgs:
        if 'src' in img.attrs:
            if 'beginner' in img['src'] and img['src'].endswith('.png'):
                print(img['src'])

    # 利用 regex 找出所有 .png 結尾且含 'beginner' 的圖片
    for img in soup.find_all('img', {'src': re.compile('beginner.*\.png$')}):
        print(img['src'])


if __name__ == '__main__':
    main()
```
# 使用regular expression的技術

>* Pythex: a Python regular expression editor    www.pythex.org

### 使用re模組

### 使用match()方法
```
import re
m = re.match(r'[a-z]+','tem123po')
print(m)

if not m==None:
    print(m.group()) #tem
    print(m.start()) #0
    print(m.end())   #3
    print(m.span())  #(0, 3)
```

### 使用search()方法

```
import re
m = re.search(r'[a-z]+','3tem12po')
print(m) # <_sre.SRE_Match object; span=(1, 4), match='tem'>
if not m==None:
    print(m.group())  # tem
    print(m.start())  # 1
    print(m.end())    # 4
    print(m.span())   # (1,4)
```


### 使用findall()方法

```
import re
m = re.findall(r'[a-z]+','3tem12po')
print(m) # ['tem', 'po']
```


### 使用compileh()方法將常用的regular expression做成物件

```
import re
reobj = re.compile(r'[a-z]+')
m = reobj.findall('3tem12po')
print(m) # ['tem', 'po']
```

### 綜合練習
```
html = """
<div class="content">
    E-Mail：<a href="mailto:mail@test.com.tw">mail</a><br>
    E-Mail2：<a href="mailto:mail2@test.com.tw">mail2</a><br>
    <ul class="price">定價：360元 </ul>
    <img src="http://test.com.tw/p1.jpg">
    <img src="http://test.com.tw/p2.jpg">
    <img src="http://test.com.tw/p3.png">
</div>
"""

import re
from bs4 import BeautifulSoup
sp = BeautifulSoup(html, 'html.parser')

emails = re.findall(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+',html)
for email in emails:
    print(email)

price=re.findall(r"[\d]+",sp.select('.price')[0].text)[0] #價格
print(price)

regex=re.compile('.*\.jpg')
imglist=sp.find_all("img",{"src":regex})
for img in imglist:
    print(img["src"])
```

# 使用selenium模組

>* pip install selenium

```
from selenium import webdriver

# 會出現 Alert 視窗
url = 'https://www.facebook.com/'
email='您的 Email 帳號'
password='您的密碼'
driver = webdriver.Chrome()

# 取消 Alert 視窗
#url = 'https://www.facebook.com/'
#chrome_options = webdriver.ChromeOptions()
#prefs = {"profile.default_content_setting_values.notifications" : 2}
#chrome_options.add_experimental_option("prefs",prefs)
#driver = webdriver.Chrome(chrome_options=chrome_options)

driver.maximize_window()
driver.get(url)

driver.find_element_by_id('email').send_keys(email)
driver.find_element_by_id('pass').send_keys(password)
driver.find_element_by_id('loginbutton').click()  # 按 登入 鈕

#driver.quit()
```

# 使用pandas模組進行分析

### 讀取各類型檔案
```
import pandas as pd
data = pd.read_csv("out.csv",encoding="utf-8-sig",index_col=0)

print(data)
```

```
import pandas as pd
data = pd.read_excel("out.xlsx",encoding="utf-8-sig",index_col=0)

print(data)
```

```
import pandas as pd
data = pd.read_html("out.html",encoding="utf-8-sig",index_col=0)

print(data[0])
```


```
import pandas as pd
data = pd.read_json("out.json", typ='series')

print(data)
```

### 輸出到各類型檔案
```
import pandas as pd
datas = [[65,92,78,83,70], [90,72,76,93,56], [81,85,91,89,77], [79,53,47,94,80]]
indexs = ["林大明", "陳聰明", "黃美麗", "熊小娟"]
columns = ["國文", "數學", "英文", "自然", "社會"]
df = pd.DataFrame(datas, columns=columns,  index=indexs)
print(df)

df.to_csv('out.csv',encoding="utf-8-sig")
```


```
import pandas as pd
datas = [[65,92,78,83,70], [90,72,76,93,56], [81,85,91,89,77], [79,53,47,94,80]]
indexs = ["林大明", "陳聰明", "黃美麗", "熊小娟"]
columns = ["國文", "數學", "英文", "自然", "社會"]
df = pd.DataFrame(datas, columns=columns,  index=indexs)
print(df)

df.to_excel('out.xlsx',encoding="utf-8-sig")
```


```
import pandas as pd
datas = [[65,92,78,83,70], [90,72,76,93,56], [81,85,91,89,77], [79,53,47,94,80]]
indexs = ["林大明", "陳聰明", "黃美麗", "熊小娟"]
columns = ["國文", "數學", "英文", "自然", "社會"]
df = pd.DataFrame(datas, columns=columns,  index=indexs)
print(df)

df.to_html('out.html')
```


```
import pandas as pd
datas = [[65,92,78,83,70], [90,72,76,93,56], [81,85,91,89,77], [79,53,47,94,80]]
indexs = ["林大明", "陳聰明", "黃美麗", "熊小娟"]
columns = ["國文", "數學", "英文", "自然", "社會"]
df = pd.DataFrame(datas, columns=columns,  index=indexs)
print(df)

df.to_json('out.json',force_ascii=False)
```


